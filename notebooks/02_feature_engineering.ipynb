{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Delivery Promise Engine: Feature Engineering\n",
        "\n",
        "## Notebook Overview\n",
        "This notebook transforms raw data into machine learning-ready features for delivery promise optimization. We merge geographic zones, weather data, temporal features, and apply robust data cleaning.\n",
        "\n",
        "### Key Transformations:\n",
        "1. **Geographic Features**: Zone mapping, borough aggregation\n",
        "2. **Temporal Features**: Hour, day-of-week, weekend, holiday flags  \n",
        "3. **Weather Integration**: Temperature, precipitation, wind speed\n",
        "4. **Data Cleaning**: Outlier filtering, missing value handling\n",
        "5. **Feature Encoding**: Categorical encoding for ML models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🔧 NYC Delivery Promise Engine - Feature Engineering\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load all datasets\n",
        "print(\"Loading datasets...\")\n",
        "df_july = pd.read_csv('../DATA/yellow_tripdata_2025-07.csv')\n",
        "df_june = pd.read_csv('../DATA/yellow_tripdata_2025-06.csv') \n",
        "df_may = pd.read_csv('../DATA/yellow_tripdata_2025-05.csv')\n",
        "\n",
        "weather = pd.read_csv('../DATA/nyc_weather_3months.csv')\n",
        "zones = pd.read_csv('../DATA/taxi_zone_lookup.csv')\n",
        "holidays = pd.read_csv('../DATA/us_public_holidays_2025.csv')\n",
        "\n",
        "print(f\"✅ Trip data: {len(df_july):,} + {len(df_june):,} + {len(df_may):,} records\")\n",
        "print(f\"✅ Weather data: {len(weather):,} records\")\n",
        "print(f\"✅ Zone lookup: {len(zones):,} zones\")\n",
        "print(f\"✅ Holidays: {len(holidays):,} holidays\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Processing and Temporal Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to process trip data and add temporal features\n",
        "def process_trip_data(df, month_name):\n",
        "    \"\"\"Process trip data with temporal feature engineering\"\"\"\n",
        "    print(f\"\\n🔄 Processing {month_name} data: {len(df):,} records\")\n",
        "    \n",
        "    # Convert datetime columns\n",
        "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    df['trip_distance'] = pd.to_numeric(df['trip_distance'], errors='coerce')\n",
        "    df['total_amount'] = pd.to_numeric(df['total_amount'], errors='coerce')\n",
        "    \n",
        "    # Calculate trip duration\n",
        "    df['trip_duration_minutes'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "    \n",
        "    # Temporal features\n",
        "    df['pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
        "    df['pickup_date_str'] = df['pickup_date'].astype(str)\n",
        "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
        "    df['pickup_dow'] = df['tpep_pickup_datetime'].dt.dayofweek  # 0=Monday\n",
        "    df['is_weekend'] = df['pickup_dow'].isin([5, 6])\n",
        "    \n",
        "    print(f\"✅ Added temporal features\")\n",
        "    return df\n",
        "\n",
        "# Process all months\n",
        "df_may = process_trip_data(df_may, \"May\")\n",
        "df_june = process_trip_data(df_june, \"June\") \n",
        "df_july = process_trip_data(df_july, \"July\")\n",
        "\n",
        "# Combine datasets (May+June for training, July for testing)\n",
        "df_train = pd.concat([df_may, df_june], ignore_index=True)\n",
        "df_test = df_july.copy()\n",
        "\n",
        "print(f\"\\n📊 Dataset Split:\")\n",
        "print(f\"Training set (May+June): {len(df_train):,} records\")\n",
        "print(f\"Test set (July): {len(df_test):,} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Geographic Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to add geographic features\n",
        "def add_geographic_features(df, zones_df):\n",
        "    \"\"\"Add pickup and dropoff zone information\"\"\"\n",
        "    print(f\"\\n🗺️ Adding geographic features...\")\n",
        "    \n",
        "    # Merge pickup zone info\n",
        "    df = df.merge(zones_df[['LocationID', 'Zone', 'Borough']], \n",
        "                  left_on='PULocationID', right_on='LocationID', how='left')\n",
        "    df.rename(columns={'Zone': 'pickup_zone', 'Borough': 'pickup_borough'}, inplace=True)\n",
        "    df.drop('LocationID', axis=1, inplace=True)\n",
        "    \n",
        "    # Merge dropoff zone info\n",
        "    df = df.merge(zones_df[['LocationID', 'Zone', 'Borough']], \n",
        "                  left_on='DOLocationID', right_on='LocationID', how='left')\n",
        "    df.rename(columns={'Zone': 'dropoff_zone', 'Borough': 'dropoff_borough'}, inplace=True)\n",
        "    df.drop('LocationID', axis=1, inplace=True)\n",
        "    \n",
        "    # Handle missing zones\n",
        "    df['pickup_zone'] = df['pickup_zone'].fillna('Unknown')\n",
        "    df['pickup_borough'] = df['pickup_borough'].fillna('Unknown')\n",
        "    df['dropoff_zone'] = df['dropoff_zone'].fillna('Unknown')\n",
        "    df['dropoff_borough'] = df['dropoff_borough'].fillna('Unknown')\n",
        "    \n",
        "    # Add route pair feature\n",
        "    df['route_pair'] = df['pickup_zone'] + ' → ' + df['dropoff_zone']\n",
        "    \n",
        "    print(f\"✅ Added zone and borough mapping\")\n",
        "    return df\n",
        "\n",
        "# Add geographic features to both datasets\n",
        "df_train = add_geographic_features(df_train, zones)\n",
        "df_test = add_geographic_features(df_test, zones)\n",
        "\n",
        "# Check top pickup/dropoff areas\n",
        "print(f\"\\n📍 Top 5 Pickup Areas:\")\n",
        "print(df_train['pickup_zone'].value_counts().head())\n",
        "\n",
        "print(f\"\\n📍 Top 5 Dropoff Areas:\")\n",
        "print(df_train['dropoff_zone'].value_counts().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning and Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply robust data cleaning\n",
        "def clean_trip_data(df):\n",
        "    \"\"\"Apply filtering rules for clean trip data\"\"\"\n",
        "    print(f\"\\n🧹 Data Cleaning: {len(df):,} records\")\n",
        "    \n",
        "    # Filter extreme duration outliers\n",
        "    min_duration = 0.5  # 30 seconds minimum\n",
        "    max_duration = df['trip_duration_minutes'].quantile(0.995)  # P99.5 threshold\n",
        "    \n",
        "    filter_mask = (\n",
        "        (df['trip_duration_minutes'] >= min_duration) & \n",
        "        (df['trip_duration_minutes'] <= max_duration) &\n",
        "        (df['trip_distance'] > 0) &\n",
        "        (df['total_amount'] > 0)\n",
        "    )\n",
        "    \n",
        "    rows_before = len(df)\n",
        "    df_clean = df[filter_mask].copy()\n",
        "    rows_after = len(df_clean)\n",
        "    rows_dropped = rows_before - rows_after\n",
        "    \n",
        "    print(f\"✅ Cleaned data: {rows_after:,} records\")\n",
        "    print(f\"📊 Removed {rows_dropped:,} outliers ({rows_dropped/rows_before:.2%})\")\n",
        "    print(f\"📈 Duration range: {df_clean['trip_duration_minutes'].min():.1f} to {df_clean['trip_duration_minutes'].max():.1f} minutes\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Clean both datasets\n",
        "df_train_clean = clean_trip_data(df_train)\n",
        "df_test_clean = clean_trip_data(df_test)\n",
        "\n",
        "# Export processed data for modeling\n",
        "print(f\"\\n💾 Exporting processed data...\")\n",
        "df_train_clean.to_csv('../data/processed_train_data.csv', index=False)\n",
        "df_test_clean.to_csv('../data/processed_test_data.csv', index=False)\n",
        "\n",
        "print(f\"✅ Training data exported: {len(df_train_clean):,} records\")\n",
        "print(f\"✅ Test data exported: {len(df_test_clean):,} records\")\n",
        "\n",
        "print(f\"\\n🎯 Feature Engineering Complete!\")\n",
        "print(f\"Ready for modeling with {len(df_train_clean):,} training records and {len(df_test_clean):,} test records.\")\n",
        "print(f\"\\n---\")\n",
        "print(f\"Continue to notebook 03_models_eta_delay.ipynb for model training.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
